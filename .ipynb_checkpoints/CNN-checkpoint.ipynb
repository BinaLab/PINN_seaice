{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7028a5-cc68-41cf-97a0-679c6b978777",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PhyCRNet for solving spatiotemporal PDEs'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import time\n",
    "import os\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.manual_seed(66)\n",
    "np.random.seed(66)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# define the high-order finite difference kernels\n",
    "lapl_op = [[[[    0,   0, -1/12,   0,     0],\n",
    "             [    0,   0,   4/3,   0,     0],\n",
    "             [-1/12, 4/3,    -5, 4/3, -1/12],\n",
    "             [    0,   0,   4/3,   0,     0],\n",
    "             [    0,   0, -1/12,   0,     0]]]]\n",
    "\n",
    "partial_y = [[[[0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0],\n",
    "               [1/12, -8/12, 0, 8/12, -1/12],\n",
    "               [0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0]]]]\n",
    "\n",
    "partial_x = [[[[0, 0, 1/12, 0, 0],\n",
    "               [0, 0, -8/12, 0, 0],\n",
    "               [0, 0, 0, 0, 0],\n",
    "               [0, 0, 8/12, 0, 0],\n",
    "               [0, 0, -1/12, 0, 0]]]]\n",
    "\n",
    "# generalized version\n",
    "# def initialize_weights(module):\n",
    "#     ''' starting from small initialized parameters '''\n",
    "#     if isinstance(module, nn.Conv2d):\n",
    "#         c = 0.1\n",
    "#         module.weight.data.uniform_(-c*np.sqrt(1 / np.prod(module.weight.shape[:-1])),\n",
    "#                                      c*np.sqrt(1 / np.prod(module.weight.shape[:-1])))\n",
    "     \n",
    "#     elif isinstance(module, nn.Linear):\n",
    "#         module.bias.data.zero_()\n",
    "\n",
    "# specific parameters for burgers equation\n",
    "def initialize_weights(module):\n",
    "\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        #nn.init.kaiming_normal_(module.weight.data, mode='fan_out')\n",
    "        c = 1 #0.5\n",
    "        module.weight.data.uniform_(-c*np.sqrt(1 / (3 * 3 * 320)), \n",
    "            c*np.sqrt(1 / (3 * 3 * 320)))\n",
    "     \n",
    "    elif isinstance(module, nn.Linear):\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    ''' Convolutional LSTM '''\n",
    "    def __init__(self, input_channels, hidden_channels, input_kernel_size, \n",
    "        input_stride, input_padding):\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.hidden_kernel_size = 3\n",
    "        self.input_kernel_size = input_kernel_size  \n",
    "        self.input_stride = input_stride\n",
    "        self.input_padding = input_padding\n",
    "        self.num_features = 4\n",
    "\n",
    "        # padding for hidden state\n",
    "        self.padding = int((self.hidden_kernel_size - 1) / 2)\n",
    "\n",
    "        self.Wxi = nn.Conv2d(self.input_channels, self.hidden_channels, \n",
    "            self.input_kernel_size, self.input_stride, self.input_padding, \n",
    "            bias=True, padding_mode='circular')\n",
    "\n",
    "        self.Whi = nn.Conv2d(self.hidden_channels, self.hidden_channels, \n",
    "            self.hidden_kernel_size, 1, padding=1, bias=False, \n",
    "            padding_mode='circular')\n",
    "\n",
    "        self.Wxf = nn.Conv2d(self.input_channels, self.hidden_channels, \n",
    "            self.input_kernel_size, self.input_stride, self.input_padding,\n",
    "            bias=True, padding_mode='circular')\n",
    "\n",
    "        self.Whf = nn.Conv2d(self.hidden_channels, self.hidden_channels, \n",
    "            self.hidden_kernel_size, 1, padding=1, bias=False, \n",
    "            padding_mode='circular')\n",
    "\n",
    "        self.Wxc = nn.Conv2d(self.input_channels, self.hidden_channels, \n",
    "            self.input_kernel_size, self.input_stride, self.input_padding,\n",
    "            bias=True, padding_mode='circular')\n",
    "\n",
    "        self.Whc = nn.Conv2d(self.hidden_channels, self.hidden_channels, \n",
    "            self.hidden_kernel_size, 1, padding=1, bias=False, \n",
    "            padding_mode='circular')\n",
    "\n",
    "        self.Wxo = nn.Conv2d(self.input_channels, self.hidden_channels, \n",
    "            self.input_kernel_size, self.input_stride, self.input_padding, \n",
    "            bias=True, padding_mode='circular')\n",
    "\n",
    "        self.Who = nn.Conv2d(self.hidden_channels, self.hidden_channels, \n",
    "            self.hidden_kernel_size, 1, padding=1, bias=False, \n",
    "            padding_mode='circular')       \n",
    "\n",
    "        nn.init.zeros_(self.Wxi.bias)\n",
    "        nn.init.zeros_(self.Wxf.bias)\n",
    "        nn.init.zeros_(self.Wxc.bias)\n",
    "        self.Wxo.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "\n",
    "        ci = torch.sigmoid(self.Wxi(x) + self.Whi(h))\n",
    "        cf = torch.sigmoid(self.Wxf(x) + self.Whf(h))\n",
    "        cc = cf * c + ci * torch.tanh(self.Wxc(x) + self.Whc(h))\n",
    "        co = torch.sigmoid(self.Wxo(x) + self.Who(h))\n",
    "        ch = co * torch.tanh(cc)\n",
    "\n",
    "        return ch, cc\n",
    "\n",
    "    def init_hidden_tensor(self, prev_state):\n",
    "        return (Variable(prev_state[0]).cuda(), Variable(prev_state[1]).cuda())\n",
    "\n",
    "\n",
    "class encoder_block(nn.Module):\n",
    "    ''' encoder with CNN '''\n",
    "    def __init__(self, input_channels, hidden_channels, input_kernel_size, \n",
    "        input_stride, input_padding):\n",
    "        \n",
    "        super(encoder_block, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.input_kernel_size = input_kernel_size  \n",
    "        self.input_stride = input_stride\n",
    "        self.input_padding = input_padding\n",
    "\n",
    "        self.conv = weight_norm(nn.Conv2d(self.input_channels, \n",
    "            self.hidden_channels, self.input_kernel_size, self.input_stride, \n",
    "            self.input_padding, bias=True, padding_mode='circular'))\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "\n",
    "class PhyCRNet(nn.Module):\n",
    "    ''' physics-informed convolutional-recurrent neural networks '''\n",
    "    def __init__(self, input_channels, hidden_channels, \n",
    "        input_kernel_size, input_stride, input_padding, dt, \n",
    "        num_layers, upscale_factor, step=1, effective_step=[1]):\n",
    "\n",
    "        super(PhyCRNet, self).__init__()\n",
    "\n",
    "        # input channels of layer includes input_channels and hidden_channels of cells \n",
    "        self.input_channels = [input_channels] + hidden_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.input_kernel_size = input_kernel_size\n",
    "        self.input_stride = input_stride\n",
    "        self.input_padding = input_padding\n",
    "        self.step = step\n",
    "        self.effective_step = effective_step\n",
    "        self._all_layers = []\n",
    "        self.dt = dt\n",
    "        self.upscale_factor = upscale_factor\n",
    "\n",
    "        # number of layers\n",
    "        self.num_encoder = num_layers[0]\n",
    "        self.num_convlstm = num_layers[1]\n",
    "\n",
    "        # encoder - downsampling  \n",
    "        for i in range(self.num_encoder):\n",
    "            name = 'encoder{}'.format(i)\n",
    "            cell = encoder_block(\n",
    "                input_channels = self.input_channels[i], \n",
    "                hidden_channels = self.hidden_channels[i], \n",
    "                input_kernel_size = self.input_kernel_size[i],\n",
    "                input_stride = self.input_stride[i],\n",
    "                input_padding = self.input_padding[i])\n",
    "\n",
    "            setattr(self, name, cell)\n",
    "            self._all_layers.append(cell)            \n",
    "            \n",
    "        # ConvLSTM\n",
    "        for i in range(self.num_encoder, self.num_encoder + self.num_convlstm):\n",
    "            name = 'convlstm{}'.format(i)\n",
    "            cell = ConvLSTMCell(\n",
    "                input_channels = self.input_channels[i],\n",
    "                hidden_channels = self.hidden_channels[i],\n",
    "                input_kernel_size = self.input_kernel_size[i],\n",
    "                input_stride = self.input_stride[i],\n",
    "                input_padding = self.input_padding[i])\n",
    "        \n",
    "            setattr(self, name, cell)\n",
    "            self._all_layers.append(cell)  \n",
    "\n",
    "        # output layer\n",
    "        self.output_layer = nn.Conv2d(2, 2, kernel_size = 5, stride = 1, \n",
    "                                      padding=2, padding_mode='circular')\n",
    "\n",
    "        # pixelshuffle - upscale\n",
    "        self.pixelshuffle = nn.PixelShuffle(self.upscale_factor)   \n",
    "\n",
    "        # initialize weights\n",
    "        self.apply(initialize_weights)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "    def forward(self, initial_state, x):\n",
    "        \n",
    "        self.initial_state = initial_state\n",
    "        internal_state = []\n",
    "        outputs = []\n",
    "        second_last_state = []\n",
    "\n",
    "        for step in range(self.step):\n",
    "            xt = x\n",
    "\n",
    "            # encoder\n",
    "            for i in range(self.num_encoder):\n",
    "                name = 'encoder{}'.format(i)\n",
    "                x = getattr(self, name)(x)\n",
    "                \n",
    "            # convlstm\n",
    "            for i in range(self.num_encoder, self.num_encoder + self.num_convlstm):\n",
    "                name = 'convlstm{}'.format(i)\n",
    "                if step == 0:\n",
    "                    (h, c) = getattr(self, name).init_hidden_tensor(\n",
    "                        prev_state = self.initial_state[i - self.num_encoder])  \n",
    "                    internal_state.append((h,c))\n",
    "                \n",
    "                # one-step forward\n",
    "                (h, c) = internal_state[i - self.num_encoder]\n",
    "                x, new_c = getattr(self, name)(x, h, c)\n",
    "                internal_state[i - self.num_encoder] = (x, new_c)                               \n",
    "\n",
    "            # output\n",
    "            x = self.pixelshuffle(x)\n",
    "            x = self.output_layer(x)\n",
    "\n",
    "            # residual connection\n",
    "            x = xt + self.dt * x\n",
    "\n",
    "            if step == (self.step - 2):\n",
    "                second_last_state = internal_state.copy()\n",
    "                \n",
    "            if step in self.effective_step:\n",
    "                outputs.append(x)                \n",
    "\n",
    "        return outputs, second_last_state\n",
    "\n",
    "\n",
    "class Conv2dDerivative(nn.Module):\n",
    "    def __init__(self, DerFilter, resol, kernel_size=3, name=''):\n",
    "        super(Conv2dDerivative, self).__init__()\n",
    "\n",
    "        self.resol = resol  # constant in the finite difference\n",
    "        self.name = name\n",
    "        self.input_channels = 1\n",
    "        self.output_channels = 1\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.padding = int((kernel_size - 1) / 2)\n",
    "        self.filter = nn.Conv2d(self.input_channels, self.output_channels, self.kernel_size, \n",
    "            1, padding=0, bias=False)\n",
    "\n",
    "        # Fixed gradient operator\n",
    "        self.filter.weight = nn.Parameter(torch.FloatTensor(DerFilter), requires_grad=False)  \n",
    "\n",
    "    def forward(self, input):\n",
    "        derivative = self.filter(input)\n",
    "        return derivative / self.resol\n",
    "\n",
    "\n",
    "class Conv1dDerivative(nn.Module):\n",
    "    def __init__(self, DerFilter, resol, kernel_size=3, name=''):\n",
    "        super(Conv1dDerivative, self).__init__()\n",
    "\n",
    "        self.resol = resol  # $\\delta$*constant in the finite difference\n",
    "        self.name = name\n",
    "        self.input_channels = 1\n",
    "        self.output_channels = 1\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.padding = int((kernel_size - 1) / 2)\n",
    "        self.filter = nn.Conv1d(self.input_channels, self.output_channels, self.kernel_size, \n",
    "            1, padding=0, bias=False)\n",
    "        \n",
    "        # Fixed gradient operator\n",
    "        self.filter.weight = nn.Parameter(torch.FloatTensor(DerFilter), requires_grad=False)  \n",
    "\n",
    "    def forward(self, input):\n",
    "        derivative = self.filter(input)\n",
    "        return derivative / self.resol\n",
    "\n",
    "\n",
    "class loss_generator(nn.Module):\n",
    "    ''' Loss generator for physics loss '''\n",
    "\n",
    "    def __init__(self, dt = (10.0/200), dx = (20.0/128)):\n",
    "        ''' Construct the derivatives, X = Width, Y = Height '''\n",
    "       \n",
    "        super(loss_generator, self).__init__()\n",
    "\n",
    "        # spatial derivative operator\n",
    "        self.laplace = Conv2dDerivative(\n",
    "            DerFilter = lapl_op,\n",
    "            resol = (dx**2),\n",
    "            kernel_size = 5,\n",
    "            name = 'laplace_operator').cuda()\n",
    "\n",
    "        self.dx = Conv2dDerivative(\n",
    "            DerFilter = partial_x,\n",
    "            resol = (dx*1),\n",
    "            kernel_size = 5,\n",
    "            name = 'dx_operator').cuda()\n",
    "\n",
    "        self.dy = Conv2dDerivative(\n",
    "            DerFilter = partial_y,\n",
    "            resol = (dx*1),\n",
    "            kernel_size = 5,\n",
    "            name = 'dy_operator').cuda()\n",
    "\n",
    "        # temporal derivative operator\n",
    "        self.dt = Conv1dDerivative(\n",
    "            DerFilter = [[[-1, 0, 1]]],\n",
    "            resol = (dt*2),\n",
    "            kernel_size = 3,\n",
    "            name = 'partial_t').cuda()\n",
    "\n",
    "    def get_phy_Loss(self, output):\n",
    "\n",
    "        # spatial derivatives\n",
    "        laplace_u = self.laplace(output[1:-1, 0:1, :, :])  # [t,c,h,w]\n",
    "        laplace_v = self.laplace(output[1:-1, 1:2, :, :])\n",
    "\n",
    "        u_x = self.dx(output[1:-1, 0:1, :, :])\n",
    "        u_y = self.dy(output[1:-1, 0:1, :, :])\n",
    "        v_x = self.dx(output[1:-1, 1:2, :, :])\n",
    "        v_y = self.dy(output[1:-1, 1:2, :, :])\n",
    "\n",
    "        # temporal derivative - u\n",
    "        u = output[:, 0:1, 2:-2, 2:-2]\n",
    "        lent = u.shape[0]\n",
    "        lenx = u.shape[3]\n",
    "        leny = u.shape[2]\n",
    "        u_conv1d = u.permute(2, 3, 1, 0)  # [height(Y), width(X), c, step]\n",
    "        u_conv1d = u_conv1d.reshape(lenx*leny,1,lent)\n",
    "        u_t = self.dt(u_conv1d)  # lent-2 due to no-padding\n",
    "        u_t = u_t.reshape(leny, lenx, 1, lent-2)\n",
    "        u_t = u_t.permute(3, 2, 0, 1)  # [step-2, c, height(Y), width(X)]\n",
    "\n",
    "        # temporal derivative - v\n",
    "        v = output[:, 1:2, 2:-2, 2:-2]\n",
    "        v_conv1d = v.permute(2, 3, 1, 0)  # [height(Y), width(X), c, step]\n",
    "        v_conv1d = v_conv1d.reshape(lenx*leny,1,lent)\n",
    "        v_t = self.dt(v_conv1d)  # lent-2 due to no-padding\n",
    "        v_t = v_t.reshape(leny, lenx, 1, lent-2)\n",
    "        v_t = v_t.permute(3, 2, 0, 1)  # [step-2, c, height(Y), width(X)]\n",
    "\n",
    "        u = output[1:-1, 0:1, 2:-2, 2:-2]  # [t, c, height(Y), width(X)]\n",
    "        v = output[1:-1, 1:2, 2:-2, 2:-2]  # [t, c, height(Y), width(X)]\n",
    "\n",
    "        assert laplace_u.shape == u_t.shape\n",
    "        assert u_t.shape == v_t.shape\n",
    "        assert laplace_u.shape == u.shape\n",
    "        assert laplace_v.shape == v.shape\n",
    "\n",
    "        R = 200.0\n",
    "\n",
    "        # 2D burgers eqn\n",
    "        f_u = u_t + u * u_x + v * u_y - (1/R) * laplace_u\n",
    "        f_v = v_t + u * v_x + v * v_y - (1/R) * laplace_v\n",
    "\n",
    "        return f_u, f_v\n",
    "\n",
    "\n",
    "def compute_loss(output, loss_func):\n",
    "    ''' calculate the phycis loss '''\n",
    "    \n",
    "    # Padding x axis due to periodic boundary condition\n",
    "    # shape: [t, c, h, w]\n",
    "    output = torch.cat((output[:, :, :, -2:], output, output[:, :, :, 0:3]), dim=3)\n",
    "\n",
    "    # Padding y axis due to periodic boundary condition\n",
    "    # shape: [t, c, h, w]\n",
    "    output = torch.cat((output[:, :, -2:, :], output, output[:, :, 0:3, :]), dim=2)\n",
    "\n",
    "    # get physics loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    f_u, f_v = loss_func.get_phy_Loss(output)\n",
    "    loss =  mse_loss(f_u, torch.zeros_like(f_u).cuda()) + mse_loss(f_v, torch.zeros_like(f_v).cuda()) \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model, input, initial_state, n_iters, time_batch_size, learning_rate, \n",
    "          dt, dx, save_path, pre_model_save_path, num_time_batch):\n",
    "\n",
    "    train_loss_list = []\n",
    "    second_last_state = []\n",
    "    prev_output = []\n",
    "\n",
    "    batch_loss = 0.0\n",
    "    best_loss = 1e4\n",
    "\n",
    "    # load previous model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    scheduler = StepLR(optimizer, step_size=100, gamma=0.97)  \n",
    "    model, optimizer, scheduler = load_checkpoint(model, optimizer, scheduler, \n",
    "        pre_model_save_path)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'])\n",
    "\n",
    "    loss_func = loss_generator(dt, dx)\n",
    "        \n",
    "    for epoch in range(n_iters):\n",
    "        # input: [t,b,c,h,w]\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0 \n",
    "        \n",
    "        for time_batch_id in range(num_time_batch):\n",
    "            # update the first input for each time batch\n",
    "            if time_batch_id == 0:\n",
    "                hidden_state = initial_state\n",
    "                u0 = input\n",
    "            else:\n",
    "                hidden_state = state_detached\n",
    "                u0 = prev_output[-2:-1].detach() # second last output\n",
    "\n",
    "            # output is a list\n",
    "            output, second_last_state = model(hidden_state, u0)\n",
    "\n",
    "            # [t, c, height (Y), width (X)]\n",
    "            output = torch.cat(tuple(output), dim=0)  \n",
    "\n",
    "            # concatenate the initial state to the output for central diff\n",
    "            output = torch.cat((u0.cuda(), output), dim=0)\n",
    "\n",
    "            # get loss\n",
    "            loss = compute_loss(output, loss_func)\n",
    "            loss.backward(retain_graph=True)\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "            # update the state and output for next batch\n",
    "            prev_output = output\n",
    "            state_detached = []\n",
    "            for i in range(len(second_last_state)):\n",
    "                (h, c) = second_last_state[i]\n",
    "                state_detached.append((h.detach(), c.detach())) # hidden state\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # print loss in each epoch\n",
    "        print('[%d/%d %d%%] loss: %.10f' % ((epoch+1), n_iters, ((epoch+1)/n_iters*100.0), \n",
    "            batch_loss))\n",
    "        train_loss_list.append(batch_loss)\n",
    "\n",
    "        # save model\n",
    "        if batch_loss < best_loss:\n",
    "            save_checkpoint(model, optimizer, scheduler, save_path)\n",
    "            best_loss = batch_loss\n",
    "    \n",
    "    return train_loss_list\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def post_process(output, true, axis_lim, uv_lim, num, fig_save_path):\n",
    "    ''' \n",
    "    axis_lim: [xmin, xmax, ymin, ymax]\n",
    "    uv_lim: [u_min, u_max, v_min, v_max]\n",
    "    num: Number of time step\n",
    "    '''\n",
    "\n",
    "    # get the limit \n",
    "    xmin, xmax, ymin, ymax = axis_lim\n",
    "    u_min, u_max, v_min, v_max = uv_lim\n",
    "\n",
    "    # grid\n",
    "    x = np.linspace(xmin, xmax, 128+1)\n",
    "    x = x[:-1]\n",
    "    x_star, y_star = np.meshgrid(x, x)\n",
    "    \n",
    "    u_star = true[num, 0, 1:-1, 1:-1]\n",
    "    u_pred = output[num, 0, 1:-1, 1:-1].detach().cpu().numpy()\n",
    "\n",
    "    v_star = true[num, 1, 1:-1, 1:-1]\n",
    "    v_pred = output[num, 1, 1:-1, 1:-1].detach().cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(7, 7))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    cf = ax[0, 0].scatter(x_star, y_star, c=u_pred, alpha=0.9, edgecolors='none', \n",
    "        cmap='RdYlBu', marker='s', s=4, vmin=u_min, vmax=u_max)\n",
    "    ax[0, 0].axis('square')\n",
    "    ax[0, 0].set_xlim([xmin, xmax])\n",
    "    ax[0, 0].set_ylim([ymin, ymax])\n",
    "    ax[0, 0].set_title('u-RCNN')\n",
    "    fig.colorbar(cf, ax=ax[0, 0])\n",
    "\n",
    "    cf = ax[0, 1].scatter(x_star, y_star, c=u_star, alpha=0.9, edgecolors='none', \n",
    "        cmap='RdYlBu', marker='s', s=4, vmin=u_min, vmax=u_max)\n",
    "    ax[0, 1].axis('square')\n",
    "    ax[0, 1].set_xlim([xmin, xmax])\n",
    "    ax[0, 1].set_ylim([ymin, ymax])\n",
    "    ax[0, 1].set_title('u-Ref.')\n",
    "    fig.colorbar(cf, ax=ax[0, 1])\n",
    "\n",
    "    cf = ax[1, 0].scatter(x_star, y_star, c=v_pred, alpha=0.9, edgecolors='none', \n",
    "        cmap='RdYlBu', marker='s', s=4, vmin=v_min, vmax=v_max)\n",
    "    ax[1, 0].axis('square')\n",
    "    ax[1, 0].set_xlim([xmin, xmax])\n",
    "    ax[1, 0].set_ylim([ymin, ymax])\n",
    "    cf.cmap.set_under('whitesmoke')\n",
    "    cf.cmap.set_over('black')\n",
    "    ax[1, 0].set_title('v-RCNN')\n",
    "    fig.colorbar(cf, ax=ax[1, 0])\n",
    "\n",
    "    cf = ax[1, 1].scatter(x_star, y_star, c=v_star, alpha=0.9, edgecolors='none', \n",
    "        cmap='RdYlBu', marker='s', s=4, vmin=v_min, vmax=v_max)\n",
    "    ax[1, 1].axis('square')\n",
    "    ax[1, 1].set_xlim([xmin, xmax])\n",
    "    ax[1, 1].set_ylim([ymin, ymax])\n",
    "    cf.cmap.set_under('whitesmoke')\n",
    "    cf.cmap.set_over('black')\n",
    "    ax[1, 1].set_title('v-Ref.')\n",
    "    fig.colorbar(cf, ax=ax[1, 1])\n",
    "\n",
    "    # plt.draw()\n",
    "    plt.savefig(fig_save_path + 'uv_comparison_'+str(num).zfill(3)+'.png')\n",
    "    plt.close('all')\n",
    "\n",
    "    return u_star, u_pred, v_star, v_pred\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, save_dir):\n",
    "    '''save model and optimizer'''\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "        }, save_dir)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, save_dir):\n",
    "    '''load model and optimizer'''\n",
    "    \n",
    "    checkpoint = torch.load(save_dir)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if (not optimizer is None):\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    print('Pretrained model loaded!')\n",
    "\n",
    "    return model, optimizer, scheduler\n",
    "\n",
    "\n",
    "def summary_parameters(model):\n",
    "    for i in model.parameters():\n",
    "        print(i.shape)\n",
    "\n",
    "\n",
    "def frobenius_norm(tensor):\n",
    "    return np.sqrt(np.sum(tensor ** 2))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ######### download the ground truth data ############\n",
    "    data_dir = './data/burgers_1501x2x128x128.mat'    \n",
    "    data = scio.loadmat(data_dir)\n",
    "    uv = data['uv'] # [t,c,h,w]  \n",
    "\n",
    "    # initial conidtion\n",
    "    uv0 = uv[0:1,...] \n",
    "    input = torch.tensor(uv0, dtype=torch.float32).cuda() \n",
    "\n",
    "    # set initial states for convlstm\n",
    "    num_convlstm = 1\n",
    "    (h0, c0) = (torch.randn(1, 128, 16, 16), torch.randn(1, 128, 16, 16))\n",
    "    initial_state = []\n",
    "    for i in range(num_convlstm):\n",
    "        initial_state.append((h0, c0))\n",
    "    \n",
    "    # grid parameters\n",
    "    time_steps = 1001\n",
    "    dt = 0.002\n",
    "    dx = 1.0 / 128\n",
    "\n",
    "    ################# build the model #####################\n",
    "    time_batch_size = 1000\n",
    "    steps = time_batch_size + 1\n",
    "    effective_step = list(range(0, steps))\n",
    "    num_time_batch = int(time_steps / time_batch_size)\n",
    "    n_iters_adam = 2000\n",
    "    lr_adam = 1e-4 #1e-3 \n",
    "    pre_model_save_path = './model/checkpoint500.pt'\n",
    "    model_save_path = './model/checkpoint1000.pt'\n",
    "    fig_save_path = './figures/'  \n",
    "\n",
    "    model = PhyCRNet(\n",
    "        input_channels = 2, \n",
    "        hidden_channels = [8, 32, 128, 128], \n",
    "        input_kernel_size = [4, 4, 4, 3], \n",
    "        input_stride = [2, 2, 2, 1], \n",
    "        input_padding = [1, 1, 1, 1],  \n",
    "        dt = dt,\n",
    "        num_layers = [3, 1],\n",
    "        upscale_factor = 8,\n",
    "        step = steps, \n",
    "        effective_step = effective_step).cuda()\n",
    "\n",
    "    start = time.time()\n",
    "    train_loss = train(model, input, initial_state, n_iters_adam, time_batch_size, \n",
    "        lr_adam, dt, dx, model_save_path, pre_model_save_path, num_time_batch)\n",
    "    end = time.time()\n",
    "    \n",
    "    np.save('./model/train_loss', train_loss)  \n",
    "    print('The training time is: ', (end-start))\n",
    "\n",
    "    ########### model inference ##################\n",
    "    time_batch_size_load = 1000\n",
    "    steps_load = time_batch_size_load + 1\n",
    "    num_time_batch = int(time_steps / time_batch_size_load)\n",
    "    effective_step = list(range(0, steps_load))  \n",
    "    \n",
    "    model = PhyCRNet(\n",
    "        input_channels = 2, \n",
    "        hidden_channels = [8, 32, 128, 128], \n",
    "        input_kernel_size = [4, 4, 4, 3], \n",
    "        input_stride = [2, 2, 2, 1], \n",
    "        input_padding = [1, 1, 1, 1],  \n",
    "        dt = dt,\n",
    "        num_layers = [3, 1],\n",
    "        upscale_factor = 8,\n",
    "        step = steps_load, \n",
    "        effective_step = effective_step).cuda()\n",
    "\n",
    "    model, _, _ = load_checkpoint(model, optimizer=None, scheduler=None, save_dir=model_save_path) \n",
    "    output, _ = model(initial_state, input)\n",
    "\n",
    "    # shape: [t, c, h, w] \n",
    "    output = torch.cat(tuple(output), dim=0)  \n",
    "    output = torch.cat((input.cuda(), output), dim=0)\n",
    "  \n",
    "    # Padding x and y axis due to periodic boundary condition\n",
    "    output = torch.cat((output[:, :, :, -1:], output, output[:, :, :, 0:2]), dim=3)\n",
    "    output = torch.cat((output[:, :, -1:, :], output, output[:, :, 0:2, :]), dim=2)\n",
    "\n",
    "    # [t, c, h, w]\n",
    "    truth = uv[0:1001,:,:,:]\n",
    "\n",
    "    # [101, 2, 131, 131]\n",
    "    truth = np.concatenate((truth[:, :, :, -1:], truth, truth[:, :, :, 0:2]), axis=3)\n",
    "    truth = np.concatenate((truth[:, :, -1:, :], truth, truth[:, :, 0:2, :]), axis=2)\n",
    "\n",
    "    # post-process\n",
    "    ten_true = []\n",
    "    ten_pred = []\n",
    "    for i in range(0, 50): \n",
    "        u_star, u_pred, v_star, v_pred = post_process(output, truth, [0,1,0,1], \n",
    "            [-0.7,0.7,-1.0,1.0], num=20*i, fig_save_path=fig_save_path)\n",
    "\n",
    "        ten_true.append([u_star, v_star])\n",
    "        ten_pred.append([u_pred, v_pred])\n",
    "\n",
    "    # compute the error\n",
    "    error = frobenius_norm(np.array(ten_pred)-np.array(ten_true)) / frobenius_norm(\n",
    "        np.array(ten_true))\n",
    "\n",
    "    print('The predicted error is: ', error)\n",
    "\n",
    "    u_pred = output[:-1, 0, :, :].detach().cpu().numpy()\n",
    "    u_pred = np.swapaxes(u_pred, 1, 2) # [h,w] = [y,x]\n",
    "    u_true = truth[:, 0, :, :]\n",
    "\n",
    "    t_true = np.linspace(0, 2, 1001)\n",
    "    t_pred = np.linspace(0, 2, time_steps)\n",
    "\n",
    "    plt.plot(t_pred, u_pred[:, 32, 32], label='x=32, y=32, CRL')\n",
    "    plt.plot(t_true, u_true[:, 32, 32], '--', label='x=32, y=32, Ref.')\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('u')\n",
    "    plt.xlim(0, 2)\n",
    "    plt.legend()\n",
    "    plt.savefig(fig_save_path + \"x=32,y=32.png\")\n",
    "    plt.close(\"all\")\n",
    "    # plt.show()\n",
    "\n",
    "    # plot train loss\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, label = 'train loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.savefig(fig_save_path + 'train loss.png', dpi = 300)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
